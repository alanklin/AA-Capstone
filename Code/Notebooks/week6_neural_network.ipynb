{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Deliverable: Neural Network\n",
    "The purpose of this is to create and implement a Neural Network modeling approach, including training and validation of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies from requirements.txt...\n",
      "All dependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "from _Setup import *\n",
    "from _Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO : Split data using Time Series Cross Validation\n",
    "This step shall be performed as it was depicted in the Week 1 Report and in the Hyndman book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data as csv\n",
    "sector_train = pd.read_csv(sector_data_csv_path_train_pivoted, index_col='Area of Responsibility')\n",
    "\n",
    "# Import the scaler\n",
    "with open(scalers_file, \"rb\") as file:\n",
    "    scalers = pickle.load(file)\n",
    "\n",
    "# DONE : Create scaled version of original Dataframe\n",
    "# Apply pre-trained scalers row-wise\n",
    "\n",
    "# Create an empty DataFrame to store scaled values\n",
    "sector_train_scaled = pd.DataFrame(index=sector_train.index, columns=sector_train.columns)\n",
    "\n",
    "# Apply each pre-trained scaler row-wise\n",
    "for index, row in sector_train.iterrows():\n",
    "\tscaler = scalers[index]\n",
    "\tscaled_row = scaler.transform(row.values.reshape(-1, 1)).flatten()\n",
    "\tsector_train_scaled.loc[index] = scaled_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE : Create a process to windowize the data for cross-validation split\n",
    "def validation_split(data, train_size = 12, validation_size = 12):\n",
    "    \"\"\"Input : Dataframe , Output : Array of Input\"\"\"\n",
    "    \"\"\"TODO : This may need to be converted to tensors\"\"\"\n",
    "\n",
    "    train_input, validation_input = [], []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        split_index = train_size\n",
    "\n",
    "        start = split_index - train_size\n",
    "        end = split_index + validation_size\n",
    "\n",
    "        train_set, validation_set = [], []\n",
    "\n",
    "        while end <= len(row):\n",
    "            train = row[start:split_index]\n",
    "            val = row[split_index:end]\n",
    "\n",
    "            train_set.append(train.values)\n",
    "            validation_set.append(val.values)\n",
    "\n",
    "            split_index += 1\n",
    "            start = split_index - train_size\n",
    "            end = split_index + validation_size\n",
    "\n",
    "        train_input.append(train_set)\n",
    "        validation_input.append(validation_set)\n",
    "\n",
    "    return train_input, validation_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 12\n",
    "train_data, val_data = validation_split(sector_train_scaled, train_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Test data\n",
    "test_data = pd.read_csv(sector_data_csv_path_test_pivoted, index_col='Area of Responsibility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for MAPE, probably something predefined but here for ease of use\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.abs((y_true - y_pred) / y_true) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25, 12), dtype=float32, numpy=\n",
       "array([[0.30925927, 0.14876543, 0.02654321, 0.0037037 , 0.        ,\n",
       "        0.20123456, 0.31851852, 1.        , 0.6240741 , 0.55987656,\n",
       "        0.37160495, 0.29938272],\n",
       "       [0.14876543, 0.02654321, 0.0037037 , 0.        , 0.20123456,\n",
       "        0.31851852, 1.        , 0.6240741 , 0.55987656, 0.37160495,\n",
       "        0.29938272, 0.24506173],\n",
       "       [0.02654321, 0.0037037 , 0.        , 0.20123456, 0.31851852,\n",
       "        1.        , 0.6240741 , 0.55987656, 0.37160495, 0.29938272,\n",
       "        0.24506173, 0.21049383],\n",
       "       [0.0037037 , 0.        , 0.20123456, 0.31851852, 1.        ,\n",
       "        0.6240741 , 0.55987656, 0.37160495, 0.29938272, 0.24506173,\n",
       "        0.21049383, 0.35555556],\n",
       "       [0.        , 0.20123456, 0.31851852, 1.        , 0.6240741 ,\n",
       "        0.55987656, 0.37160495, 0.29938272, 0.24506173, 0.21049383,\n",
       "        0.35555556, 0.24444444],\n",
       "       [0.20123456, 0.31851852, 1.        , 0.6240741 , 0.55987656,\n",
       "        0.37160495, 0.29938272, 0.24506173, 0.21049383, 0.35555556,\n",
       "        0.24444444, 0.31111112],\n",
       "       [0.31851852, 1.        , 0.6240741 , 0.55987656, 0.37160495,\n",
       "        0.29938272, 0.24506173, 0.21049383, 0.35555556, 0.24444444,\n",
       "        0.31111112, 0.36358026],\n",
       "       [1.        , 0.6240741 , 0.55987656, 0.37160495, 0.29938272,\n",
       "        0.24506173, 0.21049383, 0.35555556, 0.24444444, 0.31111112,\n",
       "        0.36358026, 0.4425926 ],\n",
       "       [0.6240741 , 0.55987656, 0.37160495, 0.29938272, 0.24506173,\n",
       "        0.21049383, 0.35555556, 0.24444444, 0.31111112, 0.36358026,\n",
       "        0.4425926 , 0.4       ],\n",
       "       [0.55987656, 0.37160495, 0.29938272, 0.24506173, 0.21049383,\n",
       "        0.35555556, 0.24444444, 0.31111112, 0.36358026, 0.4425926 ,\n",
       "        0.4       , 0.2685185 ],\n",
       "       [0.37160495, 0.29938272, 0.24506173, 0.21049383, 0.35555556,\n",
       "        0.24444444, 0.31111112, 0.36358026, 0.4425926 , 0.4       ,\n",
       "        0.2685185 , 0.33827162],\n",
       "       [0.29938272, 0.24506173, 0.21049383, 0.35555556, 0.24444444,\n",
       "        0.31111112, 0.36358026, 0.4425926 , 0.4       , 0.2685185 ,\n",
       "        0.33827162, 0.44691357],\n",
       "       [0.24506173, 0.21049383, 0.35555556, 0.24444444, 0.31111112,\n",
       "        0.36358026, 0.4425926 , 0.4       , 0.2685185 , 0.33827162,\n",
       "        0.44691357, 0.47901234],\n",
       "       [0.21049383, 0.35555556, 0.24444444, 0.31111112, 0.36358026,\n",
       "        0.4425926 , 0.4       , 0.2685185 , 0.33827162, 0.44691357,\n",
       "        0.47901234, 0.4734568 ],\n",
       "       [0.35555556, 0.24444444, 0.31111112, 0.36358026, 0.4425926 ,\n",
       "        0.4       , 0.2685185 , 0.33827162, 0.44691357, 0.47901234,\n",
       "        0.4734568 , 0.4339506 ],\n",
       "       [0.24444444, 0.31111112, 0.36358026, 0.4425926 , 0.4       ,\n",
       "        0.2685185 , 0.33827162, 0.44691357, 0.47901234, 0.4734568 ,\n",
       "        0.4339506 , 0.41049382],\n",
       "       [0.31111112, 0.36358026, 0.4425926 , 0.4       , 0.2685185 ,\n",
       "        0.33827162, 0.44691357, 0.47901234, 0.4734568 , 0.4339506 ,\n",
       "        0.41049382, 0.60493827],\n",
       "       [0.36358026, 0.4425926 , 0.4       , 0.2685185 , 0.33827162,\n",
       "        0.44691357, 0.47901234, 0.4734568 , 0.4339506 , 0.41049382,\n",
       "        0.60493827, 0.18024692],\n",
       "       [0.4425926 , 0.4       , 0.2685185 , 0.33827162, 0.44691357,\n",
       "        0.47901234, 0.4734568 , 0.4339506 , 0.41049382, 0.60493827,\n",
       "        0.18024692, 0.35555556],\n",
       "       [0.4       , 0.2685185 , 0.33827162, 0.44691357, 0.47901234,\n",
       "        0.4734568 , 0.4339506 , 0.41049382, 0.60493827, 0.18024692,\n",
       "        0.35555556, 0.22901234],\n",
       "       [0.2685185 , 0.33827162, 0.44691357, 0.47901234, 0.4734568 ,\n",
       "        0.4339506 , 0.41049382, 0.60493827, 0.18024692, 0.35555556,\n",
       "        0.22901234, 0.45123458],\n",
       "       [0.33827162, 0.44691357, 0.47901234, 0.4734568 , 0.4339506 ,\n",
       "        0.41049382, 0.60493827, 0.18024692, 0.35555556, 0.22901234,\n",
       "        0.45123458, 0.72407407],\n",
       "       [0.44691357, 0.47901234, 0.4734568 , 0.4339506 , 0.41049382,\n",
       "        0.60493827, 0.18024692, 0.35555556, 0.22901234, 0.45123458,\n",
       "        0.72407407, 0.45679012],\n",
       "       [0.47901234, 0.4734568 , 0.4339506 , 0.41049382, 0.60493827,\n",
       "        0.18024692, 0.35555556, 0.22901234, 0.45123458, 0.72407407,\n",
       "        0.45679012, 0.7839506 ],\n",
       "       [0.4734568 , 0.4339506 , 0.41049382, 0.60493827, 0.18024692,\n",
       "        0.35555556, 0.22901234, 0.45123458, 0.72407407, 0.45679012,\n",
       "        0.7839506 , 0.69814813]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index in sector_train_scaled.index:\n",
    "    \n",
    "    index_integer = sector_train_scaled.index.get_loc(index)\n",
    "\n",
    "    # Convert the first index of train_data and val_data to numpy arrays\n",
    "    split_index = 25\n",
    "    \n",
    "    train_data_np = np.array(train_data[index_integer][:split_index])\n",
    "    train_data_y_np = np.array(val_data[index_integer][:split_index])\n",
    "    \n",
    "    val_data_np = np.array(train_data[index_integer][:split_index])\n",
    "    val_data_y_np = np.array(val_data[index_integer][:split_index])\n",
    "    \n",
    "    \n",
    "    # Reshape the data to fit the model's input requirements\n",
    "    # Assuming the model expects input shape (batch_size, input_size)\n",
    "    train_data_np = train_data_np.reshape(-1, input_size)\n",
    "    train_data_y_np = train_data_y_np.reshape(-1, input_size)\n",
    "    \n",
    "    val_data_np = val_data_np.reshape(-1, input_size)\n",
    "    val_data_y_np = val_data_y_np.reshape(-1, input_size)\n",
    "    \n",
    "    # Convert numpy arrays to tensors\n",
    "    train_data_tensor = tf.convert_to_tensor(train_data_np, dtype=tf.float32)\n",
    "    train_data_y_tensor = tf.convert_to_tensor(train_data_y_np , dtype=tf.float32)\n",
    "    val_data_tensor = tf.convert_to_tensor(val_data_np, dtype=tf.float32)\n",
    "    val_data_y_tensor = tf.convert_to_tensor(val_data_np, dtype=tf.float32)\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "    # Define the neural network model\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(InputLayer(shape=(input_size,)))\n",
    "    nn_model.add(Dense(units=input_size))\n",
    "    nn_model.add(Dropout(0.3))\n",
    "    nn_model.add(Dense(units=512, activation='relu'))\n",
    "    nn_model.add(Dropout(0.3))\n",
    "    nn_model.add(Dense(units=256, activation='relu'))\n",
    "    nn_model.add(Dropout(0.3))\n",
    "    nn_model.add(Dense(units=128, activation='relu'))\n",
    "    nn_model.add(Dense(units=input_size))\n",
    "\n",
    "    nn_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    train_progress = nn_model.fit(train_data_tensor, train_data_y_tensor, epochs=50, batch_size=8, validation_data=(val_data_tensor, val_data_y_tensor))  \n",
    "\n",
    "    if index == 'Big Bend Sector':\n",
    "        # Plot the training loss\n",
    "        plt.plot(train_progress.history['loss'])\n",
    "        plt.plot(train_progress.history['val_loss'])\n",
    "        plt.title('Model Training Progress')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "    test_input = np.array(sector_train_scaled.iloc[index_integer][-input_size:]).reshape(-1, input_size)\n",
    "\n",
    "    test_input_tensor = tf.convert_to_tensor(test_input, dtype=tf.float32)\n",
    "\n",
    "    predictions = nn_model.predict(test_input_tensor)\n",
    "\n",
    "    current_sector_train = sector_train.loc[index]\n",
    "    current_sector_actual = test_data.loc[index]\n",
    "    predictions = scalers[index].inverse_transform(predictions)\n",
    "\n",
    "    # Plot the current_sector_train data\n",
    "    if index == 'Big Bend Sector':\n",
    "        plt.plot(current_sector_train.values, label=index)\n",
    "\n",
    "        # Plot the predictions data\n",
    "        plt.plot(range(len(current_sector_train), len(current_sector_train) + len(predictions.flatten())), predictions.flatten(), label='Predictions', linestyle='--')\n",
    "\n",
    "        # Plot the predictions data\n",
    "        plt.plot(range(len(current_sector_train), len(current_sector_train) + len(predictions.flatten())), current_sector_actual.values, label='Actual')\n",
    "\n",
    "        # Set the x-values as dates from the train and test sequences\n",
    "        dates = list(sector_train.columns) + list(test_data.columns)\n",
    "\n",
    "        plt.title('Current Sector Train, Validation Sets, and Predictions')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Encounters')\n",
    "        plt.xticks(ticks=range(0, len(dates), 6), labels=dates[::6], rotation=45)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE by time step: [0.6069044697984622, 8.095245041437368, 5.720592404678605, 4.304488363329752, 2.8377081452411463, 9.066412541036918, 23.94812745474727, 29.758101310773878, 17.02200551639721, 71.45447051864693, 127.70496715198864, 127.259521484375]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate MAPE for each time step\n",
    "mape_by_time_step = mean_absolute_percentage_error(current_sector_actual.values, predictions.flatten())\n",
    "\n",
    "print(f\"MAPE by time step: {mape_by_time_step.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
